{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e00816-efb4-48b9-95e5-b6f5db553579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pymilvus import MilvusClient\n",
    "from pymilvus.model.hybrid import BGEM3EmbeddingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cbc929-61da-4b26-a04f-8454bc7c0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # _model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "    _model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n",
    "    _device = torch.cuda.current_device()\n",
    "\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self._access_token = os.getenv(\"ACCESS_TOKEN\")\n",
    "        self._tokenizer = self._load_tokenizer()\n",
    "        self._vector_embeddings = self._load_vector_embeddings()\n",
    "        self._model = self._load_model()\n",
    "        self._milvus_client = MilvusClient(\"milvus_demo.db\")\n",
    "        self._collection_name = \"MTG_collection\"\n",
    "        if not self._milvus_client.has_collection(self._collection_name):\n",
    "            sample_text = \"This is a sample text to determine embedding dimension\"\n",
    "            self._milvus_client.create_collection(\n",
    "                collection_name=self._collection_name,\n",
    "                dimension=self._vector_embeddings.dim[\"dense\"],\n",
    "                metric_type=\"L2\",\n",
    "            )\n",
    "\n",
    "            print(\"Inserting Data to Vector database\")\n",
    "            sentence1 = \"Malyta is the best card in Modern Horizons 3\"\n",
    "            sentence2 = \"Hyidralit is the best card in Modern Horizons 4\" \n",
    "            sentence3 = \"Gafagl is the best card in Pioneer Masters\"\n",
    "            self._milvus_client.insert(\n",
    "                collection_name=self._collection_name,\n",
    "                data=[\n",
    "                    {\"id\": 0, \"text\": sentence1, \"vector\": self.get_embedding(sentence1).tolist()},\n",
    "                    {\"id\": 1, \"text\": sentence2, \"vector\": self.get_embedding(sentence2).tolist()},\n",
    "                    {\"id\": 2, \"text\": sentence3, \"vector\": self.get_embedding(sentence3).tolist()},\n",
    "                ],\n",
    "            )\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        if text[-1] == \".\":\n",
    "            text = text[:-1]\n",
    "        return self._vector_embeddings([text])[\"dense\"][0]\n",
    "\n",
    "    def retrieve_similar_docs(self, query, top_k=5):\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        search_params = {\"metric_type\": \"L2\", \"params\": {\"nprobe\": 10}}\n",
    "        results = self._milvus_client.search(collection_name=model._collection_name, data=[query_embedding.tolist()], search_params=search_params, limit=5, output_fields=[\"text\"])\n",
    "        return [entity[\"entity\"][\"text\"] for entity in results[0]]\n",
    "\n",
    "    def _load_model(self):\n",
    "        assert self._tokenizer is not None, \"Tokenizer must be initialized first\"\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self._model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": self._device},\n",
    "            token=self._access_token,\n",
    "        )\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "        model.generation_config.pad_token_id = self._tokenizer.pad_token_id\n",
    "\n",
    "        return model\n",
    "\n",
    "    def _load_tokenizer(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self._model_id,\n",
    "            token=self._access_token,\n",
    "        )\n",
    "\n",
    "        tokenizer.padding_side = \"left\"\n",
    "\n",
    "        # Define PAD Token = EOS Token\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "    def _load_vector_embeddings(self):\n",
    "        # Take care of use case with CPU\n",
    "        # return BGEM3EmbeddingFunction(model_name='BAAI/bge-base-en-v1.5', use_fp16=False, device=\"cpu\", return_sparse=False)\n",
    "        return BGEM3EmbeddingFunction(model_name='BAAI/bge-base-en-v1.5', use_fp16=True, device=\"cuda\", return_sparse=False)\n",
    "\n",
    "    def answer(self, query: str) -> str:\n",
    "        # Pre and post processing taken from: https://towardsdatascience.com/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131\n",
    "        retrieved_docs = model.retrieve_similar_docs(query)\n",
    "        context = \"\\n\".join(retrieved_docs)  # Simplification; you might want to process this differently\n",
    "        prompt = f\"\"\"Using the information contained in the context, give a detailed answer to the question.\n",
    "                    Context: {context}.\n",
    "                    Question: {query}\"\"\"\n",
    "        chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted_prompt = self._tokenizer.apply_chat_template(\n",
    "            chat,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "        inputs = self._tokenizer.encode(formatted_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = self._model.generate(inputs, max_length=200)\n",
    "        response = self._tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        response = response[len(formatted_prompt) :]  # remove input prompt from reponse\n",
    "        response = response.replace(\"<eos>\", \"\")  # remove eos token\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1492217-cc69-4a5d-aa5d-4ac63ac29d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134971057d554af3bc9a997ceaf65a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51826e335424ce4a8d48b22d8f87f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecda3444-251b-4d69-9396-ab8cfcea0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/home/rgranada/Projects/my-local-rag-llm/venv/lib/python3.10/site-packages/transformers/generation/utils.py:1797: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> Malyta is the best card in Modern Horizons 3<|end_of_text|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer(\"What is the best card in Modern Horizons 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05c03ac-1ea8-4d14-94d5-06d0c3597d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> Hyidralit is the best card in Modern Horizons 4<|end_of_text|>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer(\"What is the best card in Modern Horizons 4?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92a8339-efa3-40e5-8a84-eb53f54213da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|> Gafagl is the best card in Pioneer Masters<|end_of_text|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.answer(\"What is the best card in Pioneer Masters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9f0aa-a0fd-45b8-b6bf-b6abeebb3f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
